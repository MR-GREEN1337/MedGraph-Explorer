{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-09T23:02:23.547165Z",
     "iopub.status.busy": "2025-03-09T23:02:23.546804Z",
     "iopub.status.idle": "2025-03-09T23:02:26.990358Z",
     "shell.execute_reply": "2025-03-09T23:02:26.989541Z",
     "shell.execute_reply.started": "2025-03-09T23:02:23.547132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nx-arangodb in /usr/local/lib/python3.10/dist-packages (1.3.0)\n",
      "Requirement already satisfied: networkx<=3.4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from nx-arangodb) (3.4)\n",
      "Requirement already satisfied: phenolrs~=0.5 in /usr/local/lib/python3.10/dist-packages (from nx-arangodb) (0.5.9)\n",
      "Requirement already satisfied: python-arango~=8.1 in /usr/local/lib/python3.10/dist-packages (from nx-arangodb) (8.1.6)\n",
      "Requirement already satisfied: adbnx-adapter~=5.0.5 in /usr/local/lib/python3.10/dist-packages (from nx-arangodb) (5.0.6)\n",
      "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.10/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (2.32.3)\n",
      "Requirement already satisfied: rich>=12.5.1 in /usr/local/lib/python3.10/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (13.9.4)\n",
      "Requirement already satisfied: setuptools>=45 in /usr/local/lib/python3.10/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (75.1.0)\n",
      "Requirement already satisfied: numpy~=1.26 in /usr/local/lib/python3.10/dist-packages (from phenolrs~=0.5->nx-arangodb) (1.26.4)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from python-arango~=8.1->nx-arangodb) (2.3.0)\n",
      "Requirement already satisfied: requests_toolbelt in /usr/local/lib/python3.10/dist-packages (from python-arango~=8.1->nx-arangodb) (1.0.0)\n",
      "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from python-arango~=8.1->nx-arangodb) (2.10.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.7.1 in /usr/local/lib/python3.10/dist-packages (from python-arango~=8.1->nx-arangodb) (8.5.0)\n",
      "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from python-arango~=8.1->nx-arangodb) (24.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata>=4.7.1->python-arango~=8.1->nx-arangodb) (3.21.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26->phenolrs~=0.5->nx-arangodb) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26->phenolrs~=0.5->nx-arangodb) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26->phenolrs~=0.5->nx-arangodb) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (2.19.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (4.12.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (0.1.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy~=1.26->phenolrs~=0.5->nx-arangodb) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy~=1.26->phenolrs~=0.5->nx-arangodb) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nx-arangodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:02:26.992020Z",
     "iopub.status.busy": "2025-03-09T23:02:26.991725Z",
     "iopub.status.idle": "2025-03-09T23:02:27.231802Z",
     "shell.execute_reply": "2025-03-09T23:02:27.231037Z",
     "shell.execute_reply.started": "2025-03-09T23:02:26.991993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  9 23:02:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:02:27.233811Z",
     "iopub.status.busy": "2025-03-09T23:02:27.233541Z",
     "iopub.status.idle": "2025-03-09T23:02:30.633221Z",
     "shell.execute_reply": "2025-03-09T23:02:30.632447Z",
     "shell.execute_reply.started": "2025-03-09T23:02:27.233785Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Requirement already satisfied: nx-cugraph-cu12 in /usr/local/lib/python3.10/dist-packages (24.10.0)\n",
      "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from nx-cugraph-cu12) (12.2.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from nx-cugraph-cu12) (3.4)\n",
      "Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from nx-cugraph-cu12) (1.26.4)\n",
      "Requirement already satisfied: pylibcugraph-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from nx-cugraph-cu12) (24.10.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.10/dist-packages (from pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12 in /usr/local/lib/python3.10/dist-packages (from pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12 in /usr/local/lib/python3.10/dist-packages (from pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12 in /usr/local/lib/python3.10/dist-packages (from pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (12.5.4.2)\n",
      "Requirement already satisfied: pylibraft-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (24.10.0)\n",
      "Requirement already satisfied: rmm-cu12==24.10.* in /usr/local/lib/python3.10/dist-packages (from pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (24.10.0)\n",
      "Requirement already satisfied: cuda-python<13.0a0,>=12.0 in /usr/local/lib/python3.10/dist-packages (from pylibraft-cu12==24.10.*->pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (12.8.0)\n",
      "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from rmm-cu12==24.10.*->pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (0.60.0)\n",
      "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x>=12.0.0->nx-cugraph-cu12) (0.8.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0a0,>=1.23->nx-cugraph-cu12) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0a0,>=1.23->nx-cugraph-cu12) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0a0,>=1.23->nx-cugraph-cu12) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0a0,>=1.23->nx-cugraph-cu12) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12->pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (12.6.85)\n",
      "Requirement already satisfied: cuda-bindings~=12.8.0 in /usr/local/lib/python3.10/dist-packages (from cuda-python<13.0a0,>=12.0->pylibraft-cu12==24.10.*->pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (12.8.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0a0,>=1.23->nx-cugraph-cu12) (2024.2.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->rmm-cu12==24.10.*->pylibcugraph-cu12==24.10.*->nx-cugraph-cu12) (0.43.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:02:30.635208Z",
     "iopub.status.busy": "2025-03-09T23:02:30.634881Z",
     "iopub.status.idle": "2025-03-09T23:02:34.759783Z",
     "shell.execute_reply": "2025-03-09T23:02:34.758986Z",
     "shell.execute_reply.started": "2025-03-09T23:02:30.635174Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.19)\n",
      "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.3.8)\n",
      "Requirement already satisfied: langgraph in /usr/local/lib/python3.10/dist-packages (0.3.5)\n",
      "Requirement already satisfied: arango-datasets in /usr/local/lib/python3.10/dist-packages (1.2.3)\n",
      "Requirement already satisfied: kaleido in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.43)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.11.0a2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.12)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.65.5)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.10/dist-packages (from langgraph) (2.0.18)\n",
      "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from langgraph) (0.1.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.10/dist-packages (from langgraph) (0.1.55)\n",
      "Requirement already satisfied: python-arango>=7.4.1 in /usr/local/lib/python3.10/dist-packages (from arango-datasets) (8.1.6)\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from arango-datasets) (13.9.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (4.12.2)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain-community) (2.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.29.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from python-arango>=7.4.1->arango-datasets) (2.3.0)\n",
      "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from python-arango>=7.4.1->arango-datasets) (2.10.1)\n",
      "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.10/dist-packages (from python-arango>=7.4.1->arango-datasets) (75.1.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.7.1 in /usr/local/lib/python3.10/dist-packages (from python-arango>=7.4.1->arango-datasets) (8.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->arango-datasets) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->arango-datasets) (2.19.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata>=4.7.1->python-arango>=7.4.1->arango-datasets) (3.21.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->arango-datasets) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain-community) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.26.2->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.26.2->langchain-community) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade langchain langchain-community langchain-openai langgraph arango-datasets kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:02:34.760986Z",
     "iopub.status.busy": "2025-03-09T23:02:34.760756Z",
     "iopub.status.idle": "2025-03-09T23:02:34.766386Z",
     "shell.execute_reply": "2025-03-09T23:02:34.765757Z",
     "shell.execute_reply.started": "2025-03-09T23:02:34.760964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5. Import the required modules\n",
    "\n",
    "import networkx as nx\n",
    "import nx_arangodb as nxadb\n",
    "\n",
    "from arango import ArangoClient\n",
    "from arango_datasets import Datasets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import re\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.graphs import ArangoGraph\n",
    "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:02:34.767765Z",
     "iopub.status.busy": "2025-03-09T23:02:34.767464Z",
     "iopub.status.idle": "2025-03-09T23:02:34.926580Z",
     "shell.execute_reply": "2025-03-09T23:02:34.925967Z",
     "shell.execute_reply.started": "2025-03-09T23:02:34.767728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "db = ArangoClient(hosts=\"https://...arangodb.cloud:8529\").db(\"_system\", username=\"root\", password=\"...\", verify=True)\n",
    "\n",
    "#datasets = Datasets(db)\n",
    "\n",
    "#print(datasets.dataset_info(\"SYNTHEA_P100\"))\n",
    "\n",
    "#datasets.load(\"SYNTHEA_P100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:02:34.927643Z",
     "iopub.status.busy": "2025-03-09T23:02:34.927332Z",
     "iopub.status.idle": "2025-03-09T23:02:35.812192Z",
     "shell.execute_reply": "2025-03-09T23:02:35.811341Z",
     "shell.execute_reply.started": "2025-03-09T23:02:34.927614Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f9f4fb6dbf', 'finish_reason': 'stop', 'logprobs': None}, id='run-9761357b-6e2b-4e95-8a96-15f2ec8aad76-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\"\"\"os.environ[\"GROQ_API_KEY\"] = \"gsk_...\"\n",
    "\n",
    "llm = ChatGroq(temperature=0, model_name=\"deepseek-r1-distill-qwen-32b\")\n",
    "\"\"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "\n",
    "llm.invoke(\"hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:02:35.814874Z",
     "iopub.status.busy": "2025-03-09T23:02:35.814632Z",
     "iopub.status.idle": "2025-03-09T23:03:41.584909Z",
     "shell.execute_reply": "2025-03-09T23:03:41.584171Z",
     "shell.execute_reply.started": "2025-03-09T23:02:35.814851Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:02:35 +0000] [INFO]: Graph 'SYNTHEA_P100' exists.\n",
      "[23:02:35 +0000] [INFO]: Default node type set to 'allergies'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph loaded with 145514 nodes and 311701 edges\n",
      "GPU acceleration enabled via cuGraph\n"
     ]
    }
   ],
   "source": [
    "G_adb = nxadb.Graph(name=\"SYNTHEA_P100\", db=db, write_batch_size=100_000)\n",
    "print(f\"Graph loaded with {len(G_adb.nodes)} nodes and {len(G_adb.edges)} edges\")\n",
    "\n",
    "# Optional: try to init cuGraph for GPU acceleration\n",
    "try:\n",
    "    import nx_cugraph as nxcg\n",
    "    G_cugraph = nxcg.Graph(G_adb)\n",
    "    use_gpu = True\n",
    "    print(\"GPU acceleration enabled via cuGraph\")\n",
    "except Exception as e:\n",
    "    use_gpu = False\n",
    "    print(f\"GPU acceleration not available: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:03:41.586310Z",
     "iopub.status.busy": "2025-03-09T23:03:41.586023Z",
     "iopub.status.idle": "2025-03-09T23:03:41.595842Z",
     "shell.execute_reply": "2025-03-09T23:03:41.595032Z",
     "shell.execute_reply.started": "2025-03-09T23:03:41.586286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def query_medical_graph(query_spec: dict):\n",
    "    \"\"\"\n",
    "    Dynamic medical graph analysis tool that determines the optimal query technique\n",
    "    and executes it. Supports AQL, NetworkX/cuGraph algorithms, or hybrid approaches.\n",
    "    \n",
    "    Parameters:\n",
    "    - query_spec: A dictionary containing:\n",
    "      - query: Natural language query about medical data\n",
    "      - context: Optional additional context\n",
    "      - parameters: Optional specific query parameters\n",
    "      - approach: Optional preferred analysis method\n",
    "    \"\"\"\n",
    "    # Extract query components\n",
    "    query = query_spec['query']\n",
    "    context = query_spec.get('context', {})\n",
    "    parameters = query_spec.get('parameters', {})\n",
    "    approach = query_spec.get('approach', None)\n",
    "    \n",
    "    # Analyze query intent to determine approach\n",
    "    query_analysis = llm.invoke(f\"\"\"\n",
    "    Analyze this medical query intent: \"{query}\"\n",
    "    \n",
    "    Classify this query into one of the following categories:\n",
    "    1. SIMPLE_RELATIONSHIP - Direct lookups, basic traversals, simple filtering\n",
    "    2. COMPLEX_PATTERN - Requires graph algorithms (centrality, community detection, path analysis)\n",
    "    3. HYBRID - Requires both relationship data and complex graph analytics\n",
    "    \n",
    "    Return classification in JSON:\n",
    "    {{\n",
    "        \"category\": \"SIMPLE_RELATIONSHIP|COMPLEX_PATTERN|HYBRID\",\n",
    "        \"explanation\": \"Brief explanation why\",\n",
    "        \"suggested_approach\": \"AQL|NetworkX|Hybrid\"\n",
    "    }}\n",
    "    \n",
    "    Context: {context if context else \"No additional context provided\"}\n",
    "    \"\"\").content\n",
    "    \n",
    "    # Extract analysis results\n",
    "    try:\n",
    "        analysis_result = json.loads(re.search(r'\\{.*\\}', query_analysis, re.DOTALL).group())\n",
    "        query_category = analysis_result.get('category', 'SIMPLE_RELATIONSHIP')\n",
    "        suggested_approach = analysis_result.get('suggested_approach', 'AQL')\n",
    "    except:\n",
    "        query_category = 'SIMPLE_RELATIONSHIP'\n",
    "        suggested_approach = 'AQL'\n",
    "    \n",
    "    # Override with specified approach if provided\n",
    "    if approach:\n",
    "        suggested_approach = approach\n",
    "    \n",
    "    # Execute appropriate query technique\n",
    "    if suggested_approach == 'AQL' or query_category == 'SIMPLE_RELATIONSHIP':\n",
    "        return execute_aql_query(query, parameters, context)\n",
    "    elif suggested_approach == 'NetworkX' or query_category == 'COMPLEX_PATTERN':\n",
    "        return execute_networkx_query(query, parameters, context)\n",
    "    else:  # Hybrid approach\n",
    "        return execute_hybrid_query(query, parameters, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:03:41.596932Z",
     "iopub.status.busy": "2025-03-09T23:03:41.596628Z",
     "iopub.status.idle": "2025-03-09T23:03:41.622899Z",
     "shell.execute_reply": "2025-03-09T23:03:41.622079Z",
     "shell.execute_reply.started": "2025-03-09T23:03:41.596865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def execute_aql_query(query, parameters={}, context={}):\n",
    "    \"\"\"Execute AQL queries via LangChain for simple relationship queries\"\"\"\n",
    "    chain = ArangoGraphQAChain.from_llm(\n",
    "        llm=llm,\n",
    "        graph=arango_graph,\n",
    "        verbose=True,\n",
    "        allow_dangerous_requests=True\n",
    "    )\n",
    "    \n",
    "    # Enhance query with medical context\n",
    "    enhanced_query = f\"\"\"\n",
    "    Based on the Synthea medical knowledge graph:\n",
    "    \n",
    "    Original query: {query}\n",
    "    \n",
    "    Additional context: {context if context else \"None\"}\n",
    "    \n",
    "    Parameters to consider: {parameters if parameters else \"None\"}\n",
    "    \"\"\"\n",
    "    \n",
    "    result = chain.invoke(enhanced_query)\n",
    "    \n",
    "    # Structure and analyze results\n",
    "    analysis = llm.invoke(f\"\"\"\n",
    "    Based on this query result from a medical database:\n",
    "    \n",
    "    {result['result']}\n",
    "    \n",
    "    Extract and structure the key information in a format useful for medical analysis.\n",
    "    Focus on presenting clear, structured data highlighting any potential rare disease insights.\n",
    "    \n",
    "    Return a structured JSON with relevant medical fields.\n",
    "    \"\"\").content\n",
    "    \n",
    "    try:\n",
    "        structured_data = json.loads(re.search(r'\\{.*\\}', analysis, re.DOTALL).group())\n",
    "        return {\n",
    "            \"result\": result['result'],\n",
    "            \"structured_data\": structured_data,\n",
    "            \"query_type\": \"AQL\",\n",
    "            \"original_query\": query\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            \"result\": result['result'],\n",
    "            \"query_type\": \"AQL\",\n",
    "            \"original_query\": query\n",
    "        }\n",
    "\n",
    "def execute_networkx_query(query, parameters={}, context={}):\n",
    "    \"\"\"Execute NetworkX/cuGraph algorithms for complex pattern analysis\"\"\"\n",
    "    # Generate appropriate NetworkX/cuGraph code\n",
    "    code_generation_prompt = f\"\"\"\n",
    "    I have a NetworkX Graph `G_adb` representing a medical knowledge graph from Synthea.\n",
    "    \n",
    "    The query is: \"{query}\"\n",
    "    \n",
    "    Additional context: {context if context else \"None\"}\n",
    "    Parameters: {parameters if parameters else \"None\"}\n",
    "    \n",
    "    Generate Python code using NetworkX and/or cuGraph algorithms to answer this query.\n",
    "    Focus on detecting rare disease patterns, unusual symptom clusters, or atypical progressions.\n",
    "    \n",
    "    Your code should:\n",
    "    1. Extract relevant subgraph if needed\n",
    "    2. Apply appropriate graph algorithms (centrality, community detection, path analysis)\n",
    "    3. Interpret results for rare disease analysis\n",
    "    4. Store final answer in variable FINAL_RESULT\n",
    "    \n",
    "    Only provide executable Python code without explanations.\n",
    "    \"\"\"\n",
    "\n",
    "    nx_code = llm.invoke(code_generation_prompt).content\n",
    "    nx_code_cleaned = re.sub(r\"^```python\\n|```$\", \"\", nx_code, flags=re.MULTILINE).strip()\n",
    "    \n",
    "    # Try to use GPU acceleration when available\n",
    "    try:\n",
    "        if use_gpu:\n",
    "            global_vars = {\"G_adb\": G_adb, \"G_cugraph\": G_cugraph, \"nx\": nx, \"nxcg\": nxcg, \"db\": db}\n",
    "        else:\n",
    "            global_vars = {\"G_adb\": G_adb, \"nx\": nx, \"db\": db}\n",
    "        \n",
    "        local_vars = {}\n",
    "        exec(nx_code_cleaned, global_vars, local_vars)\n",
    "        result = local_vars.get(\"FINAL_RESULT\", \"No result was generated\")\n",
    "    except Exception as e:\n",
    "        # Try to fix common errors\n",
    "        fix_prompt = f\"\"\"\n",
    "        The following NetworkX code failed with error: {str(e)}\n",
    "        \n",
    "        Code:\n",
    "        {nx_code_cleaned}\n",
    "        \n",
    "        Please fix the code to address this error. Focus only on fixing the error.\n",
    "        \"\"\"\n",
    "        \n",
    "        fixed_code = llm.invoke(fix_prompt).content\n",
    "        fixed_code_cleaned = re.sub(r\"^```python\\n|```$\", \"\", fixed_code, flags=re.MULTILINE).strip()\n",
    "        \n",
    "        try:\n",
    "            exec(fixed_code_cleaned, global_vars, local_vars)\n",
    "            result = local_vars.get(\"FINAL_RESULT\", \"No result was generated after fixing code\")\n",
    "        except Exception as e2:\n",
    "            return {\n",
    "                \"error\": f\"Graph analysis failed: {str(e2)}\",\n",
    "                \"query_type\": \"NetworkX\",\n",
    "                \"original_query\": query\n",
    "            }\n",
    "    \n",
    "    # Enhance the result with medical interpretation\n",
    "    result_interpretation = llm.invoke(f\"\"\"\n",
    "    I executed graph analytics on a medical knowledge graph to answer: \"{query}\"\n",
    "    \n",
    "    The analysis result is: {result}\n",
    "    \n",
    "    Please interpret this result in the context of rare disease diagnosis, explaining:\n",
    "    1. What patterns or insights were found\n",
    "    2. How these relate to potential rare disease diagnosis\n",
    "    3. Clinical significance of these findings\n",
    "    \n",
    "    Return your interpretation in a structured JSON format with medical insights.\n",
    "    \"\"\").content\n",
    "    \n",
    "    try:\n",
    "        interpretation_data = json.loads(re.search(r'\\{.*\\}', result_interpretation, re.DOTALL).group())\n",
    "        return {\n",
    "            \"result\": result,\n",
    "            \"interpretation\": interpretation_data,\n",
    "            \"query_type\": \"NetworkX\",\n",
    "            \"original_query\": query\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            \"result\": result,\n",
    "            \"interpretation\": result_interpretation,\n",
    "            \"query_type\": \"NetworkX\",\n",
    "            \"original_query\": query\n",
    "        }\n",
    "\n",
    "def execute_hybrid_query(query, parameters={}, context={}):\n",
    "    \"\"\"Execute hybrid queries combining AQL and NetworkX for complex analysis\"\"\"\n",
    "    # Determine what parts need AQL vs NetworkX\n",
    "    query_decomposition = llm.invoke(f\"\"\"\n",
    "    I need to analyze this medical query using a hybrid approach combining AQL and NetworkX:\n",
    "    \n",
    "    \"{query}\"\n",
    "    \n",
    "    Break this query into components that should be handled by:\n",
    "    1. AQL - for retrieving specific data relationships\n",
    "    2. NetworkX - for complex pattern analysis\n",
    "    \n",
    "    Provide decomposition in JSON format:\n",
    "    {{\n",
    "        \"aql_component\": \"What specific data should be retrieved using AQL\",\n",
    "        \"networkx_component\": \"What analysis should be performed using NetworkX\",\n",
    "        \"integration_strategy\": \"How to combine the results\"\n",
    "    }}\n",
    "    \"\"\").content\n",
    "    \n",
    "    try:\n",
    "        decomposition = json.loads(re.search(r'\\{.*\\}', query_decomposition, re.DOTALL).group())\n",
    "        aql_component = decomposition.get('aql_component', '')\n",
    "        networkx_component = decomposition.get('networkx_component', '')\n",
    "        integration_strategy = decomposition.get('integration_strategy', '')\n",
    "    except:\n",
    "        # Default decomposition if parsing fails\n",
    "        aql_component = f\"Retrieve relevant data for: {query}\"\n",
    "        networkx_component = f\"Analyze patterns in the data for: {query}\"\n",
    "        integration_strategy = \"Combine the results to provide insights\"\n",
    "    \n",
    "    # Step 1: Execute AQL query to get base data\n",
    "    aql_result = execute_aql_query(aql_component, parameters, context)\n",
    "    \n",
    "    # Step 2: Prepare NetworkX analysis with context from AQL results\n",
    "    enhanced_context = {\n",
    "        **context,\n",
    "        \"aql_results\": aql_result.get(\"result\", \"\")\n",
    "    }\n",
    "    \n",
    "    # Step 3: Execute NetworkX analysis\n",
    "    networkx_result = execute_networkx_query(networkx_component, parameters, enhanced_context)\n",
    "    \n",
    "    # Step 4: Integrate results\n",
    "    integration_prompt = f\"\"\"\n",
    "    I executed a hybrid analysis on a medical knowledge graph using both AQL and NetworkX:\n",
    "    \n",
    "    Original query: \"{query}\"\n",
    "    \n",
    "    AQL component result: {aql_result.get('result', '')}\n",
    "    \n",
    "    NetworkX component result: {networkx_result.get('result', '')}\n",
    "    \n",
    "    Integration strategy: {integration_strategy}\n",
    "    \n",
    "    Please integrate these results to provide a comprehensive answer.\n",
    "    Focus on insights related to rare disease patterns, unusual symptoms, or diagnostic pathways.\n",
    "    \n",
    "    Return your integrated analysis in a structured JSON format with medical insights.\n",
    "    \"\"\"\n",
    "    \n",
    "    integrated_result = llm.invoke(integration_prompt).content\n",
    "    \n",
    "    try:\n",
    "        integrated_data = json.loads(re.search(r'\\{.*\\}', integrated_result, re.DOTALL).group())\n",
    "        return {\n",
    "            \"result\": integrated_result,\n",
    "            \"structured_data\": integrated_data,\n",
    "            \"aql_component\": aql_result,\n",
    "            \"networkx_component\": networkx_result,\n",
    "            \"query_type\": \"Hybrid\",\n",
    "            \"original_query\": query\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            \"result\": integrated_result,\n",
    "            \"aql_component\": aql_result,\n",
    "            \"networkx_component\": networkx_result,\n",
    "            \"query_type\": \"Hybrid\",\n",
    "            \"original_query\": query\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:03:41.624029Z",
     "iopub.status.busy": "2025-03-09T23:03:41.623746Z",
     "iopub.status.idle": "2025-03-09T23:03:41.651729Z",
     "shell.execute_reply": "2025-03-09T23:03:41.650872Z",
     "shell.execute_reply.started": "2025-03-09T23:03:41.623994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def analyze_patient_symptoms(patient_spec: dict):\n",
    "    \"\"\"\n",
    "    Advanced patient symptom analyzer for rare disease identification.\n",
    "    \n",
    "    Parameters:\n",
    "    - patient_spec: A dictionary containing:\n",
    "      - patient_id: Optional patient ID\n",
    "      - symptoms: Optional list of symptoms to analyze\n",
    "      - metadata: Optional additional information\n",
    "      - analysis_type: Analysis type (\"rare_disease\", \"evidence_path\", \n",
    "                       \"similar_cases\", \"progression\")\n",
    "    \"\"\"\n",
    "    # Extract patient specification\n",
    "    patient_id = patient_spec.get('patient_id')\n",
    "    symptoms = patient_spec.get('symptoms', [])\n",
    "    metadata = patient_spec.get('metadata', {})\n",
    "    analysis_type = patient_spec.get('analysis_type', 'rare_disease')\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Symptom-based analysis (no patient ID)\n",
    "    if not patient_id and symptoms:\n",
    "        # Convert symptoms to codes if needed\n",
    "        symptom_codes = []\n",
    "        for symptom in symptoms:\n",
    "            if not str(symptom).isdigit():\n",
    "                # Find SNOMED codes for symptom descriptions\n",
    "                aql_query = f\"\"\"\n",
    "                FOR obs IN observations\n",
    "                    FILTER LOWER(obs.DESCRIPTION) LIKE LOWER(\"%{symptom}%\")\n",
    "                    RETURN DISTINCT obs.CODE\n",
    "                \"\"\"\n",
    "                cursor = db.aql.execute(aql_query)\n",
    "                codes = [doc for doc in cursor]\n",
    "                if codes:\n",
    "                    symptom_codes.extend(codes)\n",
    "            else:\n",
    "                symptom_codes.append(symptom)\n",
    "        \n",
    "        # Find rare conditions associated with these symptoms\n",
    "        aql_query = f\"\"\"\n",
    "        LET symptom_codes = {symptom_codes}\n",
    "        \n",
    "        // Find patients with these symptoms\n",
    "        LET patients_with_symptoms = (\n",
    "            FOR obs IN observations\n",
    "                FILTER obs.CODE IN symptom_codes\n",
    "                RETURN DISTINCT obs.PATIENT\n",
    "        )\n",
    "        \n",
    "        // Find conditions these patients have\n",
    "        LET conditions = (\n",
    "            FOR patient IN patients_with_symptoms\n",
    "                FOR cond IN conditions\n",
    "                    FILTER cond.PATIENT == patient\n",
    "                    COLLECT code = cond.CODE, description = cond.DESCRIPTION\n",
    "                    WITH COUNT INTO count\n",
    "                    SORT count ASC\n",
    "                    RETURN {{\n",
    "                        code: code,\n",
    "                        description: description,\n",
    "                        patient_count: count,\n",
    "                        prevalence: count / LENGTH(patients_with_symptoms)\n",
    "                    }}\n",
    "        )\n",
    "        \n",
    "        // Return relatively rare conditions\n",
    "        FOR cond IN conditions\n",
    "            FILTER cond.patient_count <= 0.1 * LENGTH(patients_with_symptoms)\n",
    "            SORT cond.patient_count ASC\n",
    "            LIMIT 10\n",
    "            RETURN cond\n",
    "        \"\"\"\n",
    "        \n",
    "        cursor = db.aql.execute(aql_query)\n",
    "        results[\"potential_rare_conditions\"] = [doc for doc in cursor]\n",
    "        results[\"symptom_analysis\"] = {\n",
    "            \"input_symptoms\": symptoms,\n",
    "            \"symptom_codes_identified\": symptom_codes,\n",
    "            \"analysis_type\": \"symptom-based\"\n",
    "        }\n",
    "    \n",
    "    # Patient ID-based analysis\n",
    "    elif patient_id:\n",
    "        # Get patient observations\n",
    "        aql_query = f\"\"\"\n",
    "        FOR obs IN observations\n",
    "            FILTER obs.PATIENT == \"{patient_id}\"\n",
    "            SORT obs.DATE\n",
    "            RETURN {{\n",
    "                code: obs.CODE,\n",
    "                description: obs.DESCRIPTION,\n",
    "                date: obs.DATE,\n",
    "                value: obs.VALUE\n",
    "            }}\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql_query)\n",
    "        patient_observations = [doc for doc in cursor]\n",
    "        \n",
    "        # Get patient conditions\n",
    "        aql_query = f\"\"\"\n",
    "        FOR c IN conditions\n",
    "            FILTER c.PATIENT == \"{patient_id}\"\n",
    "            RETURN {{\n",
    "                code: c.CODE,\n",
    "                description: c.DESCRIPTION,\n",
    "                start: c.START,\n",
    "                stop: c.STOP\n",
    "            }}\n",
    "        \"\"\"\n",
    "        cursor = db.aql.execute(aql_query)\n",
    "        patient_conditions = [doc for doc in cursor]\n",
    "        \n",
    "        results[\"patient_data\"] = {\n",
    "            \"observations\": patient_observations,\n",
    "            \"conditions\": patient_conditions,\n",
    "            \"patient_id\": patient_id\n",
    "        }\n",
    "        \n",
    "        # Analyze based on analysis type\n",
    "        if analysis_type == \"evidence_path\":\n",
    "            # Find rarest condition if not specified\n",
    "            if \"condition_id\" not in metadata:\n",
    "                aql_query = f\"\"\"\n",
    "                FOR c IN conditions\n",
    "                    FILTER c.PATIENT == \"{patient_id}\"\n",
    "                    LET condition_count = (\n",
    "                        FOR c2 IN conditions\n",
    "                            FILTER c2.CODE == c.CODE\n",
    "                            COLLECT WITH COUNT INTO count\n",
    "                            RETURN count\n",
    "                    )\n",
    "                    SORT condition_count[0] ASC\n",
    "                    LIMIT 1\n",
    "                    RETURN {{\n",
    "                        code: c.CODE,\n",
    "                        description: c.DESCRIPTION,\n",
    "                        count: condition_count[0]\n",
    "                    }}\n",
    "                \"\"\"\n",
    "                cursor = db.aql.execute(aql_query)\n",
    "                rare_condition = [doc for doc in cursor][0] if cursor else None\n",
    "                \n",
    "                if rare_condition:\n",
    "                    metadata[\"condition_id\"] = rare_condition[\"code\"]\n",
    "            \n",
    "            # Create evidence path visualization\n",
    "            if \"condition_id\" in metadata:\n",
    "                condition_id = metadata[\"condition_id\"]\n",
    "                # Create a subgraph showing evidence path\n",
    "                G = nx.DiGraph()\n",
    "                \n",
    "                # Get condition information\n",
    "                condition_info = None\n",
    "                for c in patient_conditions:\n",
    "                    if str(c[\"code\"]) == str(condition_id):\n",
    "                        condition_info = c\n",
    "                        break\n",
    "                \n",
    "                if condition_info:\n",
    "                    # Add condition node\n",
    "                    G.add_node(f\"condition_{condition_info['code']}\", \n",
    "                              label=condition_info['description'],\n",
    "                              type=\"condition\",\n",
    "                              date=condition_info['start'])\n",
    "                    \n",
    "                    # Add observation nodes and edges\n",
    "                    for obs in patient_observations:\n",
    "                        if \"date\" in obs:\n",
    "                            G.add_node(f\"obs_{obs['code']}_{obs['date']}\", \n",
    "                                      label=obs['description'],\n",
    "                                      value=obs.get('value', 'N/A'),\n",
    "                                      type=\"observation\",\n",
    "                                      date=obs['date'])\n",
    "                            \n",
    "                            # Connect if observation came before diagnosis\n",
    "                            if obs['date'] <= condition_info['start']:\n",
    "                                G.add_edge(f\"obs_{obs['code']}_{obs['date']}\", \n",
    "                                          f\"condition_{condition_info['code']}\")\n",
    "                    \n",
    "                    # Store graph information\n",
    "                    results[\"evidence_path\"] = {\n",
    "                        \"condition\": condition_info,\n",
    "                        \"node_count\": len(G.nodes()),\n",
    "                        \"edge_count\": len(G.edges()),\n",
    "                        \"graph\": G\n",
    "                    }\n",
    "                \n",
    "        elif analysis_type == \"similar_cases\":\n",
    "            # Find similar patients with rare diseases\n",
    "            patient_obs_codes = [obs[\"code\"] for obs in patient_observations]\n",
    "            \n",
    "            # Get all patients and their observations\n",
    "            aql_query = f\"\"\"\n",
    "            LET all_patients = (\n",
    "                FOR p IN patients\n",
    "                    RETURN DISTINCT p._key\n",
    "            )\n",
    "            \n",
    "            FOR patient_id IN all_patients\n",
    "                LET patient_obs = (\n",
    "                    FOR obs IN observations\n",
    "                        FILTER obs.PATIENT == patient_id\n",
    "                        RETURN DISTINCT obs.CODE\n",
    "                )\n",
    "                \n",
    "                LET rare_conditions = (\n",
    "                    FOR c IN conditions\n",
    "                        FILTER c.PATIENT == patient_id\n",
    "                        // Count occurrences of this condition\n",
    "                        LET condition_count = (\n",
    "                            FOR c2 IN conditions\n",
    "                                FILTER c2.CODE == c.CODE\n",
    "                                COLLECT WITH COUNT INTO count\n",
    "                                RETURN count\n",
    "                        )\n",
    "                        // Only keep conditions in <5% of patients\n",
    "                        FILTER condition_count[0] <= 0.05 * LENGTH(all_patients)\n",
    "                        RETURN {{\n",
    "                            code: c.CODE,\n",
    "                            description: c.DESCRIPTION\n",
    "                        }}\n",
    "                )\n",
    "                \n",
    "                // Only include patients with rare conditions\n",
    "                FILTER LENGTH(rare_conditions) > 0\n",
    "                \n",
    "                // Calculate Jaccard similarity\n",
    "                LET jaccard_similarity = LENGTH(INTERSECTION(patient_obs, {patient_obs_codes})) / \n",
    "                                      LENGTH(UNION(patient_obs, {patient_obs_codes}))\n",
    "                                      \n",
    "                SORT jaccard_similarity DESC\n",
    "                LIMIT {metadata.get('top_k', 5) + 1}  // +1 for patient themselves\n",
    "                \n",
    "                RETURN {{\n",
    "                    patient_id: patient_id,\n",
    "                    similarity: jaccard_similarity,\n",
    "                    rare_conditions: rare_conditions,\n",
    "                    shared_observation_count: LENGTH(INTERSECTION(patient_obs, {patient_obs_codes})),\n",
    "                    total_observation_count: LENGTH(patient_obs)\n",
    "                }}\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor = db.aql.execute(aql_query)\n",
    "            similar_patients = [doc for doc in cursor]\n",
    "            \n",
    "            # Filter out query patient\n",
    "            similar_patients = [p for p in similar_patients if p['patient_id'] != patient_id][:metadata.get('top_k', 5)]\n",
    "            \n",
    "            results[\"similar_cases\"] = {\n",
    "                \"similar_patients\": similar_patients,\n",
    "                \"query_patient_id\": patient_id\n",
    "            }\n",
    "            \n",
    "        elif analysis_type == \"progression\":\n",
    "            # Analyze disease progression patterns\n",
    "            timeline = []\n",
    "            \n",
    "            # Add observations to timeline\n",
    "            for obs in patient_observations:\n",
    "                if \"date\" in obs:\n",
    "                    timeline.append({\n",
    "                        \"date\": obs[\"date\"],\n",
    "                        \"event_type\": \"observation\",\n",
    "                        \"description\": obs[\"description\"],\n",
    "                        \"code\": obs[\"code\"],\n",
    "                        \"value\": obs.get(\"value\", \"\")\n",
    "                    })\n",
    "            \n",
    "            # Add conditions to timeline\n",
    "            for cond in patient_conditions:\n",
    "                if \"start\" in cond:\n",
    "                    timeline.append({\n",
    "                        \"date\": cond[\"start\"],\n",
    "                        \"event_type\": \"condition_start\",\n",
    "                        \"description\": cond[\"description\"],\n",
    "                        \"code\": cond[\"code\"]\n",
    "                    })\n",
    "                \n",
    "                if \"stop\" in cond and cond[\"stop\"]:\n",
    "                    timeline.append({\n",
    "                        \"date\": cond[\"stop\"],\n",
    "                        \"event_type\": \"condition_end\",\n",
    "                        \"description\": cond[\"description\"],\n",
    "                        \"code\": cond[\"code\"]\n",
    "                    })\n",
    "            \n",
    "            # Sort timeline by date\n",
    "            timeline.sort(key=lambda x: x[\"date\"])\n",
    "            \n",
    "            results[\"progression_analysis\"] = {\n",
    "                \"timeline\": timeline,\n",
    "                \"patient_id\": patient_id\n",
    "            }\n",
    "        \n",
    "        else:  # Default to rare disease analysis\n",
    "            # Identify rare conditions for this patient\n",
    "            aql_query = f\"\"\"\n",
    "            LET all_patients = (\n",
    "                FOR p IN patients\n",
    "                    RETURN DISTINCT p._key\n",
    "            )\n",
    "            \n",
    "            FOR c IN conditions\n",
    "                FILTER c.PATIENT == \"{patient_id}\"\n",
    "                // Get count of this condition across patients\n",
    "                LET condition_count = (\n",
    "                    FOR c2 IN conditions\n",
    "                        FILTER c2.CODE == c.CODE\n",
    "                        COLLECT WITH COUNT INTO count\n",
    "                        RETURN count\n",
    "                )\n",
    "                // Calculate rarity\n",
    "                LET rarity = 1 - (condition_count[0] / LENGTH(all_patients))\n",
    "                // Only return rare conditions\n",
    "                FILTER rarity >= 0.9\n",
    "                SORT rarity DESC\n",
    "                RETURN {{\n",
    "                    code: c.CODE,\n",
    "                    description: c.DESCRIPTION,\n",
    "                    rarity: rarity,\n",
    "                    patient_count: condition_count[0],\n",
    "                    total_patients: LENGTH(all_patients)\n",
    "                }}\n",
    "            \"\"\"\n",
    "            \n",
    "            cursor = db.aql.execute(aql_query)\n",
    "            rare_conditions = [doc for doc in cursor]\n",
    "            \n",
    "            results[\"rare_disease_analysis\"] = {\n",
    "                \"rare_conditions\": rare_conditions,\n",
    "                \"patient_id\": patient_id\n",
    "            }\n",
    "    \n",
    "    # Enhance results with LLM insights\n",
    "    analysis_prompt = f\"\"\"\n",
    "    I'm analyzing medical data for potential rare disease patterns:\n",
    "    \n",
    "    {results}\n",
    "    \n",
    "    Provide a clinical interpretation in the context of rare disease diagnosis.\n",
    "    Focus on:\n",
    "    1. Key patterns or insights\n",
    "    2. Potential rare disease indications\n",
    "    3. Suggested next steps for investigation\n",
    "    \n",
    "    Format as JSON with structured fields for different aspects.\n",
    "    \"\"\"\n",
    "    \n",
    "    analysis_result = llm.invoke(analysis_prompt).content\n",
    "    \n",
    "    try:\n",
    "        interpretation = json.loads(re.search(r'\\{.*\\}', analysis_result, re.DOTALL).group())\n",
    "        results[\"interpretation\"] = interpretation\n",
    "    except:\n",
    "        results[\"interpretation\"] = analysis_result\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:03:41.652811Z",
     "iopub.status.busy": "2025-03-09T23:03:41.652606Z",
     "iopub.status.idle": "2025-03-09T23:03:41.674567Z",
     "shell.execute_reply": "2025-03-09T23:03:41.673943Z",
     "shell.execute_reply.started": "2025-03-09T23:03:41.652792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def generate_medical_visualization(visualization_spec: dict):\n",
    "    \"\"\"\n",
    "    High-quality medical visualization generator that adapts to data type.\n",
    "    \n",
    "    Parameters:\n",
    "    - visualization_spec: A dictionary with:\n",
    "      - data: Data to visualize\n",
    "      - type: Visualization type (\"network\", \"timeline\", \"heatmap\", \"auto\")\n",
    "      - title: Title for the visualization\n",
    "      - context: Contextual information for visualization\n",
    "    \"\"\"\n",
    "    # Extract visualization specification\n",
    "    data = visualization_spec.get('data', {})\n",
    "    viz_type = visualization_spec.get('type', 'auto')\n",
    "    title = visualization_spec.get('title', 'Medical Data Visualization')\n",
    "    context = visualization_spec.get('context', {})\n",
    "    \n",
    "    # Determine best visualization if auto\n",
    "    if viz_type == 'auto':\n",
    "        if isinstance(data, nx.Graph) or 'graph' in data or ('nodes' in data and 'edges' in data):\n",
    "            viz_type = 'network'\n",
    "        elif 'timeline' in data or any('date' in str(item) for item in data):\n",
    "            viz_type = 'timeline'\n",
    "        elif 'matrix' in data or 'heatmap' in data:\n",
    "            viz_type = 'heatmap'\n",
    "        else:\n",
    "            viz_type = 'network'  # Default for medical data\n",
    "    \n",
    "    # Create appropriate visualization\n",
    "    if viz_type == 'network':\n",
    "        return create_network_visualization(data, title, context)\n",
    "    elif viz_type == 'timeline':\n",
    "        return create_timeline_visualization(data, title, context)\n",
    "    elif viz_type == 'heatmap':\n",
    "        return create_heatmap_visualization(data, title, context)\n",
    "    elif viz_type == 'sankey':\n",
    "        return create_sankey_visualization(data, title, context)\n",
    "    else:\n",
    "        return {\"error\": f\"Unsupported visualization type: {viz_type}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:03:41.675477Z",
     "iopub.status.busy": "2025-03-09T23:03:41.675268Z",
     "iopub.status.idle": "2025-03-09T23:03:41.708086Z",
     "shell.execute_reply": "2025-03-09T23:03:41.707245Z",
     "shell.execute_reply.started": "2025-03-09T23:03:41.675453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def create_network_visualization(data, title, context):\n",
    "    \"\"\"Create network visualization using Matplotlib instead of Plotly\"\"\"\n",
    "    # Extract or create graph structure\n",
    "    if isinstance(data, nx.Graph):\n",
    "        G = data\n",
    "    elif 'graph' in data and isinstance(data['graph'], nx.Graph):\n",
    "        G = data['graph']\n",
    "    elif 'nodes' in data and 'edges' in data:\n",
    "        G = nx.Graph()\n",
    "        for node in data['nodes']:\n",
    "            G.add_node(node['id'], **{k: v for k, v in node.items() if k != 'id'})\n",
    "        for edge in data['edges']:\n",
    "            G.add_edge(edge['source'], edge['target'], **{k: v for k, v in edge.items() \n",
    "                                                       if k not in ['source', 'target']})\n",
    "    else:\n",
    "        # Create a default medical knowledge graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add some nodes and edges based on context\n",
    "        condition = context.get('condition', 'Unknown Condition')\n",
    "        G.add_node('condition', label=condition, type='condition')\n",
    "        \n",
    "        symptoms = context.get('symptoms', ['Symptom 1', 'Symptom 2', 'Symptom 3'])\n",
    "        for i, symptom in enumerate(symptoms):\n",
    "            G.add_node(f'symptom_{i}', label=symptom, type='symptom')\n",
    "            G.add_edge(f'symptom_{i}', 'condition', type='indicates')\n",
    "    \n",
    "    # Get positions for nodes using spring layout\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Map node types to colors\n",
    "    color_map = {\n",
    "        'condition': 'red',\n",
    "        'symptom': 'blue',\n",
    "        'observation': 'blue',\n",
    "        'medication': 'green',\n",
    "        'procedure': 'purple',\n",
    "        'allergy': 'orange',\n",
    "        'patient': 'yellow',\n",
    "        'unknown': 'gray'\n",
    "    }\n",
    "    \n",
    "    # Group nodes by type for better visualization\n",
    "    node_types = {}\n",
    "    for node in G.nodes():\n",
    "        node_type = G.nodes[node].get('type', 'unknown')\n",
    "        if node_type not in node_types:\n",
    "            node_types[node_type] = []\n",
    "        node_types[node_type].append(node)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.3, edge_color='gray')\n",
    "    \n",
    "    # Draw nodes by type with different colors\n",
    "    for node_type, nodes in node_types.items():\n",
    "        color = color_map.get(node_type, 'gray')\n",
    "        nx.draw_networkx_nodes(G, pos, \n",
    "                              nodelist=nodes, \n",
    "                              node_color=color,\n",
    "                              node_size=500,\n",
    "                              alpha=0.8,\n",
    "                              label=node_type)\n",
    "    \n",
    "    # Add labels\n",
    "    node_labels = {}\n",
    "    for node in G.nodes():\n",
    "        if 'label' in G.nodes[node]:\n",
    "            node_labels[node] = G.nodes[node]['label']\n",
    "        else:\n",
    "            node_labels[node] = str(node)\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=10, font_color='black')\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add title and remove axes\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Save visualization to file\n",
    "    img_path = f\"{title.replace(' ', '_').lower()}.png\"\n",
    "    plt.savefig(img_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        \"image_path\": img_path,\n",
    "        \"node_count\": len(G.nodes()),\n",
    "        \"edge_count\": len(G.edges()),\n",
    "        \"visualization_type\": \"network\"\n",
    "    }\n",
    "def create_timeline_visualization(data, title, context):\n",
    "    \"\"\"Create interactive timeline visualization for disease progression\"\"\"\n",
    "    # Extract timeline data\n",
    "    if 'timeline' in data:\n",
    "        timeline_data = data['timeline']\n",
    "    else:\n",
    "        timeline_data = data\n",
    "    \n",
    "    # Ensure timeline_data is a list\n",
    "    if not isinstance(timeline_data, list):\n",
    "        return {\"error\": \"Timeline data must be a list of events\"}\n",
    "    \n",
    "    # Sort timeline by date if not already sorted\n",
    "    timeline_data.sort(key=lambda x: x.get('date', ''))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    df = pd.DataFrame(timeline_data)\n",
    "    \n",
    "    # If dataframe doesn't have certain columns, add defaults\n",
    "    if 'date' not in df.columns:\n",
    "        return {\"error\": \"Timeline data must contain 'date' field\"}\n",
    "    \n",
    "    if 'event_type' not in df.columns:\n",
    "        df['event_type'] = 'event'\n",
    "        \n",
    "    if 'description' not in df.columns:\n",
    "        df['description'] = 'Unknown event'\n",
    "    \n",
    "    # Map event types to colors\n",
    "    color_map = {\n",
    "        'condition_start': 'red',\n",
    "        'condition_end': 'orange',\n",
    "        'observation': 'blue',\n",
    "        'medication_start': 'green',\n",
    "        'medication_end': 'lightgreen',\n",
    "        'procedure': 'purple',\n",
    "        'event': 'gray'\n",
    "    }\n",
    "    \n",
    "    # Create interactive timeline using Plotly\n",
    "    fig = px.scatter(df, x='date', y='event_type', \n",
    "                   color='event_type', hover_name='description',\n",
    "                   color_discrete_map=color_map,\n",
    "                   title=title)\n",
    "    \n",
    "    # Add details to hover\n",
    "    hover_template = \"<b>%{hovertext}</b><br>Date: %{x}<br>Type: %{y}<br>\"\n",
    "    if 'value' in df.columns:\n",
    "        hover_template += \"Value: %{customdata}<br>\"\n",
    "        fig.update_traces(customdata=df['value'])\n",
    "    if 'code' in df.columns:\n",
    "        hover_template += \"Code: %{customdata}<br>\"\n",
    "        fig.update_traces(customdata=df['code'])\n",
    "        \n",
    "    fig.update_traces(hovertemplate=hover_template)\n",
    "    \n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"Time\",\n",
    "        yaxis_title=\"Event Type\",\n",
    "        legend_title=\"Event Type\",\n",
    "        template=\"plotly_white\",\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    # Add connecting lines for progression\n",
    "    fig.update_layout(\n",
    "        shapes=[\n",
    "            dict(\n",
    "                type=\"line\",\n",
    "                xref=\"x\", yref=\"paper\",\n",
    "                x0=df['date'][i], y0=0, \n",
    "                x1=df['date'][i], y1=1,\n",
    "                line=dict(\n",
    "                    color=\"Gray\",\n",
    "                    width=1,\n",
    "                    dash=\"dot\",\n",
    "                )\n",
    "            )\n",
    "            for i in range(len(df))\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Save visualization to HTML file\n",
    "    output_path = f\"{title.replace(' ', '_').lower()}.html\"\n",
    "    fig.write_html(output_path)\n",
    "    \n",
    "    # Also save a static image\n",
    "    img_path = f\"{title.replace(' ', '_').lower()}.png\"\n",
    "    fig.write_image(img_path)\n",
    "    \n",
    "    return {\n",
    "        \"visualization_path\": output_path,\n",
    "        \"image_path\": img_path,\n",
    "        \"event_count\": len(timeline_data),\n",
    "        \"visualization_type\": \"timeline\"\n",
    "    }\n",
    "\n",
    "def create_heatmap_visualization(data, title, context):\n",
    "    \"\"\"Create heatmap visualization for symptom-disease relationships\"\"\"\n",
    "    # Extract heatmap data\n",
    "    if 'matrix' in data:\n",
    "        matrix_data = data['matrix']\n",
    "        row_labels = data.get('row_labels', [])\n",
    "        col_labels = data.get('col_labels', [])\n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        matrix_data = data.values\n",
    "        row_labels = data.index.tolist()\n",
    "        col_labels = data.columns.tolist()\n",
    "    else:\n",
    "        # Try to construct a heatmap from available data\n",
    "        matrix_data = []\n",
    "        row_labels = []\n",
    "        col_labels = []\n",
    "        \n",
    "        # This is a fallback if proper matrix data isn't provided\n",
    "        if context.get('symptoms') and context.get('conditions'):\n",
    "            symptoms = context.get('symptoms', [])\n",
    "            conditions = context.get('conditions', [])\n",
    "            \n",
    "            # Create an empty matrix\n",
    "            matrix_data = np.zeros((len(symptoms), len(conditions)))\n",
    "            row_labels = symptoms\n",
    "            col_labels = conditions\n",
    "            \n",
    "            # Fill with random data as placeholder\n",
    "            for i in range(len(symptoms)):\n",
    "                for j in range(len(conditions)):\n",
    "                    matrix_data[i][j] = np.random.random()\n",
    "        else:\n",
    "            # Complete fallback with placeholder data\n",
    "            symptoms = [\"Symptom 1\", \"Symptom 2\", \"Symptom 3\", \"Symptom 4\", \"Symptom 5\"]\n",
    "            conditions = [\"Disease A\", \"Disease B\", \"Disease C\", \"Disease D\", \"Disease E\"]\n",
    "            \n",
    "            matrix_data = np.random.rand(len(symptoms), len(conditions))\n",
    "            row_labels = symptoms\n",
    "            col_labels = conditions\n",
    "    \n",
    "    # Create heatmap visualization using Plotly\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=matrix_data,\n",
    "        x=col_labels,\n",
    "        y=row_labels,\n",
    "        colorscale='Blues',\n",
    "        hovertemplate='%{y}  %{x}: %{z:.3f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis=dict(title='Conditions'),\n",
    "        yaxis=dict(title='Symptoms'),\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    \n",
    "    # Save visualization to HTML file\n",
    "    output_path = f\"{title.replace(' ', '_').lower()}.html\"\n",
    "    fig.write_html(output_path)\n",
    "    \n",
    "    # Also save a static image\n",
    "    img_path = f\"{title.replace(' ', '_').lower()}.png\"\n",
    "    fig.write_image(img_path)\n",
    "    \n",
    "    return {\n",
    "        \"visualization_path\": output_path,\n",
    "        \"image_path\": img_path,\n",
    "        \"matrix_dimensions\": f\"{len(matrix_data)}x{len(matrix_data[0]) if matrix_data else 0}\",\n",
    "        \"visualization_type\": \"heatmap\"\n",
    "    }\n",
    "\n",
    "def create_sankey_visualization(data, title, context):\n",
    "    \"\"\"Create Sankey diagram for patient pathways or disease progression\"\"\"\n",
    "    # Prepare data for Sankey diagram\n",
    "    if 'links' in data and 'nodes' in data:\n",
    "        links = data['links']\n",
    "        nodes = data['nodes']\n",
    "    else:\n",
    "        # Try to construct from pathway data if available\n",
    "        links = []\n",
    "        nodes = []\n",
    "        node_ids = {}\n",
    "        \n",
    "        # Extract pathway data\n",
    "        pathways = data.get('pathways', [])\n",
    "        if not pathways and 'timeline' in data:\n",
    "            # Convert timeline to pathways\n",
    "            events = data['timeline']\n",
    "            events.sort(key=lambda x: x.get('date', ''))\n",
    "            \n",
    "            # Group by patient if applicable\n",
    "            patient_id = context.get('patient_id', 'unknown')\n",
    "            pathways = [{\n",
    "                'patient_id': patient_id,\n",
    "                'events': events\n",
    "            }]\n",
    "        \n",
    "        # Create nodes and links from pathways\n",
    "        node_counter = 0\n",
    "        for pathway in pathways:\n",
    "            events = pathway.get('events', [])\n",
    "            \n",
    "            # Add nodes for each unique event\n",
    "            for event in events:\n",
    "                event_type = event.get('event_type', 'event')\n",
    "                description = event.get('description', 'Unknown')\n",
    "                \n",
    "                # Create a unique node identifier\n",
    "                node_id = f\"{event_type}_{description}\"\n",
    "                \n",
    "                if node_id not in node_ids:\n",
    "                    node_ids[node_id] = node_counter\n",
    "                    nodes.append({\n",
    "                        'id': node_counter,\n",
    "                        'name': description,\n",
    "                        'type': event_type\n",
    "                    })\n",
    "                    node_counter += 1\n",
    "            \n",
    "            # Add links between sequential events\n",
    "            for i in range(len(events) - 1):\n",
    "                source_event = events[i]\n",
    "                target_event = events[i + 1]\n",
    "                \n",
    "                source_id = node_ids[f\"{source_event.get('event_type', 'event')}_{source_event.get('description', 'Unknown')}\"]\n",
    "                target_id = node_ids[f\"{target_event.get('event_type', 'event')}_{target_event.get('description', 'Unknown')}\"]\n",
    "                \n",
    "                # Check if link already exists and increment value if so\n",
    "                link_exists = False\n",
    "                for link in links:\n",
    "                    if link['source'] == source_id and link['target'] == target_id:\n",
    "                        link['value'] += 1\n",
    "                        link_exists = True\n",
    "                        break\n",
    "                \n",
    "                if not link_exists:\n",
    "                    links.append({\n",
    "                        'source': source_id,\n",
    "                        'target': target_id,\n",
    "                        'value': 1\n",
    "                    })\n",
    "    \n",
    "    # Create Sankey diagram\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        node=dict(\n",
    "            pad=15,\n",
    "            thickness=20,\n",
    "            line=dict(color=\"black\", width=0.5),\n",
    "            label=[node.get('name', f\"Node {node['id']}\") for node in nodes],\n",
    "            color=[get_color_for_type(node.get('type', 'unknown')) for node in nodes]\n",
    "        ),\n",
    "        link=dict(\n",
    "            source=[link['source'] for link in links],\n",
    "            target=[link['target'] for link in links],\n",
    "            value=[link['value'] for link in links]\n",
    "        )\n",
    "    )])\n",
    "    \n",
    "    # Customize layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        font=dict(size=12),\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    \n",
    "    # Save visualization to HTML file\n",
    "    output_path = f\"{title.replace(' ', '_').lower()}.html\"\n",
    "    fig.write_html(output_path)\n",
    "    \n",
    "    # Also save a static image\n",
    "    img_path = f\"{title.replace(' ', '_').lower()}.png\"\n",
    "    fig.write_image(img_path)\n",
    "    \n",
    "    return {\n",
    "        \"visualization_path\": output_path,\n",
    "        \"image_path\": img_path,\n",
    "        \"node_count\": len(nodes),\n",
    "        \"link_count\": len(links),\n",
    "        \"visualization_type\": \"sankey\"\n",
    "    }\n",
    "\n",
    "def get_color_for_type(event_type):\n",
    "    \"\"\"Helper function to get colors based on event type\"\"\"\n",
    "    color_map = {\n",
    "        'condition': 'rgba(255, 0, 0, 0.8)',\n",
    "        'condition_start': 'rgba(255, 0, 0, 0.8)',\n",
    "        'condition_end': 'rgba(255, 150, 0, 0.8)',\n",
    "        'observation': 'rgba(0, 0, 255, 0.8)',\n",
    "        'medication': 'rgba(0, 128, 0, 0.8)',\n",
    "        'medication_start': 'rgba(0, 128, 0, 0.8)',\n",
    "        'medication_end': 'rgba(144, 238, 144, 0.8)',\n",
    "        'procedure': 'rgba(128, 0, 128, 0.8)',\n",
    "        'symptom': 'rgba(0, 191, 255, 0.8)',\n",
    "        'allergy': 'rgba(255, 165, 0, 0.8)',\n",
    "        'patient': 'rgba(255, 255, 0, 0.8)',\n",
    "        'event': 'rgba(128, 128, 128, 0.8)',\n",
    "        'unknown': 'rgba(128, 128, 128, 0.8)'\n",
    "    }\n",
    "    return color_map.get(event_type.lower(), 'rgba(128, 128, 128, 0.8)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:03:41.709043Z",
     "iopub.status.busy": "2025-03-09T23:03:41.708838Z",
     "iopub.status.idle": "2025-03-09T23:03:41.727160Z",
     "shell.execute_reply": "2025-03-09T23:03:41.726480Z",
     "shell.execute_reply.started": "2025-03-09T23:03:41.709024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_rare_disease_agent():\n",
    "    \"\"\"Create an agent with the custom medical tools\"\"\"\n",
    "    tools = [\n",
    "        query_medical_graph,\n",
    "        analyze_patient_symptoms,\n",
    "        generate_medical_visualization\n",
    "    ]\n",
    "    \n",
    "    # Create the agent with LangGraph\n",
    "    agent = create_react_agent(\n",
    "        llm, \n",
    "        tools\n",
    "    )\n",
    "    \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:03:41.728163Z",
     "iopub.status.busy": "2025-03-09T23:03:41.727883Z",
     "iopub.status.idle": "2025-03-09T23:03:41.755028Z",
     "shell.execute_reply": "2025-03-09T23:03:41.754304Z",
     "shell.execute_reply.started": "2025-03-09T23:03:41.728132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rare_disease_agent = create_rare_disease_agent()\n",
    "\n",
    "# Define helper functions to query the agent\n",
    "def query_graph(query):\n",
    "    \"\"\"Run a query through the medical graph agent\"\"\"\n",
    "    response = rare_disease_agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
    "    return response[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:03:41.756313Z",
     "iopub.status.busy": "2025-03-09T23:03:41.756032Z",
     "iopub.status.idle": "2025-03-09T23:03:41.776206Z",
     "shell.execute_reply": "2025-03-09T23:03:41.775444Z",
     "shell.execute_reply.started": "2025-03-09T23:03:41.756285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_improved_dashboard():\n",
    "    \"\"\"Create a simplified, more intuitive dashboard for medical graph exploration\"\"\"\n",
    "    # Import necessary libraries\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import clear_output, display, HTML\n",
    "    import json\n",
    "    import os\n",
    "    import re\n",
    "    \n",
    "    # Create main layout with improved styling\n",
    "    header = widgets.HTML(\"\"\"\n",
    "    <div style=\"background-color: #1A5276; color: white; padding: 20px; border-radius: 5px; margin-bottom: 20px;\">\n",
    "        <h1 style=\"margin: 0;\">MedGraph Explorer</h1>\n",
    "        <p>Interactive Medical Graph Analysis for Rare Disease Patterns</p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create simplified chat interface\n",
    "    chat_input = widgets.Text(\n",
    "        placeholder='Ask a question about medical data or patient conditions...',\n",
    "        description='',\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "    \n",
    "    send_button = widgets.Button(\n",
    "        description='Send',\n",
    "        button_style='primary',\n",
    "        icon='paper-plane',\n",
    "        layout=widgets.Layout(width='15%')\n",
    "    )\n",
    "    \n",
    "    input_area = widgets.HBox([chat_input, send_button])\n",
    "    \n",
    "    # Create conversation display area\n",
    "    conversation_area = widgets.Output(\n",
    "        layout=widgets.Layout(\n",
    "            width='100%',\n",
    "            height='500px',\n",
    "            border='1px solid #ddd',\n",
    "            overflow_y='auto'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Create preset queries for quick access\n",
    "    preset_queries = widgets.Dropdown(\n",
    "        options=[\n",
    "            'Show me common symptoms of autoimmune disorders',\n",
    "            'Find patients with rare conditions',\n",
    "            'Identify disease clusters in the patient population',\n",
    "            'Show progression timeline for lupus patients',\n",
    "            'Visualize diabetes and heart disease relationships',\n",
    "            'What are the most common comorbidities for rare diseases?'\n",
    "        ],\n",
    "        description='Quick Queries:',\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "    \n",
    "    use_preset_button = widgets.Button(\n",
    "        description='Use Query',\n",
    "        button_style='info',\n",
    "        icon='lightbulb-o',\n",
    "        layout=widgets.Layout(width='15%')\n",
    "    )\n",
    "    \n",
    "    preset_area = widgets.HBox([preset_queries, use_preset_button])\n",
    "    \n",
    "    # Patient search section\n",
    "    patient_search = widgets.Text(\n",
    "        placeholder='Enter patient ID for detailed analysis',\n",
    "        description='Patient ID:',\n",
    "        layout=widgets.Layout(width='70%')\n",
    "    )\n",
    "    \n",
    "    search_button = widgets.Button(\n",
    "        description='Analyze',\n",
    "        button_style='success',\n",
    "        icon='search',\n",
    "        layout=widgets.Layout(width='25%')\n",
    "    )\n",
    "    \n",
    "    patient_search_area = widgets.HBox([patient_search, search_button])\n",
    "    \n",
    "    # Data refresh and status indicators\n",
    "    status_indicator = widgets.HTML(\n",
    "        value='<span style=\"color:green; font-weight:bold;\"></span> System Ready',\n",
    "        layout=widgets.Layout(width='70%')\n",
    "    )\n",
    "    \n",
    "    refresh_button = widgets.Button(\n",
    "        description='Refresh Data',\n",
    "        button_style='warning',\n",
    "        icon='refresh',\n",
    "        layout=widgets.Layout(width='25%')\n",
    "    )\n",
    "    \n",
    "    status_area = widgets.HBox([status_indicator, refresh_button])\n",
    "    \n",
    "    # Assemble the dashboard\n",
    "    dashboard = widgets.VBox([\n",
    "        header,\n",
    "        widgets.HTML('<h3>Medical Intelligence Chat</h3>'),\n",
    "        conversation_area,\n",
    "        input_area,\n",
    "        widgets.HTML('<hr>'),\n",
    "        widgets.HTML('<h3>Quick Actions</h3>'),\n",
    "        preset_area,\n",
    "        patient_search_area,\n",
    "        widgets.HTML('<hr>'),\n",
    "        status_area\n",
    "    ])\n",
    "    \n",
    "    # Function to add messages to conversation\n",
    "    def add_message(role, content):\n",
    "        with conversation_area:\n",
    "            # Style based on role\n",
    "            if role == \"user\":\n",
    "                display(HTML(f'<div style=\"margin: 10px; text-align: right;\"><span style=\"background-color: #2E86C1; color: white; padding: 8px 12px; border-radius: 15px; display: inline-block; max-width: 80%; text-align: left;\"><strong>You:</strong> {content}</span></div>'))\n",
    "            elif role == \"assistant\":\n",
    "                display(HTML(f'<div style=\"margin: 10px; text-align: left;\"><span style=\"background-color: #34495E; color: white; padding: 8px 12px; border-radius: 15px; display: inline-block; max-width: 80%;\"><strong>MedGraph:</strong> {content}</span></div>'))\n",
    "            elif role == \"system\":\n",
    "                display(HTML(f'<div style=\"margin: 10px; text-align: center;\"><span style=\"color: #777; font-style: italic;\">{content}</span></div>'))\n",
    "            elif role == \"visualization\":\n",
    "                display(HTML(f'<div style=\"margin: 10px; text-align: center;\"><div style=\"border: 1px solid #ddd; padding: 10px; display: inline-block;\">{content}</div></div>'))\n",
    "    \n",
    "    # Function to process user query\n",
    "    def process_query(query):\n",
    "        add_message(\"system\", \"Processing query...\")\n",
    "        try:\n",
    "            # Update status\n",
    "            status_indicator.value = '<span style=\"color:orange; font-weight:bold;\"></span> Processing query...'\n",
    "            \n",
    "            # Run query through agent\n",
    "            result = rare_disease_agent.invoke({\n",
    "                \"messages\": [{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": query\n",
    "                }]\n",
    "            })\n",
    "            \n",
    "            # Extract response\n",
    "            response = result[\"messages\"][-1].content\n",
    "            \n",
    "            # Check for visualization paths in the response\n",
    "            viz_html = \"\"\n",
    "            try:\n",
    "                if \"visualization_path\" in response or \"image_path\" in response:\n",
    "                    viz_path_match = re.search(r'\"(visualization_path|image_path)\":\\s*\"([^\"]+)\"', response)\n",
    "                    if viz_path_match and os.path.exists(viz_path_match.group(2)):\n",
    "                        viz_path = viz_path_match.group(2)\n",
    "                        if viz_path.endswith('.html'):\n",
    "                            viz_html = f'<iframe src=\"{viz_path}\" width=\"100%\" height=\"400px\"></iframe>'\n",
    "                        else:\n",
    "                            viz_html = f'<img src=\"{viz_path}\" style=\"max-width:100%;\">'\n",
    "                \n",
    "                # Clean up response text by removing file paths\n",
    "                clean_response = re.sub(r'\"(visualization_path|image_path)\":\\s*\"[^\"]+\"', '', response)\n",
    "                clean_response = re.sub(r'{[\\s\\S]*}', '', clean_response).strip()\n",
    "                \n",
    "                # Display response\n",
    "                add_message(\"assistant\", clean_response)\n",
    "                \n",
    "                # Display visualization if available\n",
    "                if viz_html:\n",
    "                    add_message(\"visualization\", viz_html)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing visualization: {e}\")\n",
    "            \n",
    "            # Reset status\n",
    "            status_indicator.value = '<span style=\"color:green; font-weight:bold;\"></span> Ready for next query'\n",
    "            \n",
    "        except Exception as e:\n",
    "            add_message(\"system\", f\"Error: {str(e)}\")\n",
    "            status_indicator.value = '<span style=\"color:red; font-weight:bold;\"></span> Error occurred'\n",
    "    \n",
    "    # Function to analyze patient\n",
    "    def analyze_patient(patient_id):\n",
    "        if not patient_id.strip():\n",
    "            add_message(\"system\", \"Please enter a valid patient ID\")\n",
    "            return\n",
    "        \n",
    "        add_message(\"system\", f\"Analyzing patient {patient_id}...\")\n",
    "        try:\n",
    "            # Update status\n",
    "            status_indicator.value = '<span style=\"color:orange; font-weight:bold;\"></span> Analyzing patient data...'\n",
    "            \n",
    "            # Patient analysis parameters\n",
    "            patient_params = {\n",
    "                'patient_id': patient_id,\n",
    "                'analysis_type': 'rare_disease'  # Default to rare disease analysis\n",
    "            }\n",
    "            \n",
    "            # Run patient analysis\n",
    "            result = rare_disease_agent.invoke({\n",
    "                \"messages\": [{\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": f\"Perform comprehensive analysis on patient {patient_id}. Look for rare conditions, unique symptom patterns, and provide clinical insights.\"\n",
    "                }]\n",
    "            })\n",
    "            \n",
    "            # Extract response\n",
    "            response = result[\"messages\"][-1].content\n",
    "            \n",
    "            # Check for visualization paths in the response\n",
    "            viz_html = \"\"\n",
    "            try:\n",
    "                if \"visualization_path\" in response or \"image_path\" in response:\n",
    "                    viz_path_match = re.search(r'\"(visualization_path|image_path)\":\\s*\"([^\"]+)\"', response)\n",
    "                    if viz_path_match and os.path.exists(viz_path_match.group(2)):\n",
    "                        viz_path = viz_path_match.group(2)\n",
    "                        if viz_path.endswith('.html'):\n",
    "                            viz_html = f'<iframe src=\"{viz_path}\" width=\"100%\" height=\"400px\"></iframe>'\n",
    "                        else:\n",
    "                            viz_html = f'<img src=\"{viz_path}\" style=\"max-width:100%;\">'\n",
    "                \n",
    "                # Clean up response text\n",
    "                clean_response = re.sub(r'\"(visualization_path|image_path)\":\\s*\"[^\"]+\"', '', response)\n",
    "                clean_response = re.sub(r'{[\\s\\S]*}', '', clean_response).strip()\n",
    "                \n",
    "                # Display response\n",
    "                add_message(\"assistant\", clean_response)\n",
    "                \n",
    "                # Display visualization if available\n",
    "                if viz_html:\n",
    "                    add_message(\"visualization\", viz_html)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing visualization: {e}\")\n",
    "            \n",
    "            # Reset status\n",
    "            status_indicator.value = '<span style=\"color:green; font-weight:bold;\"></span> Patient analysis complete'\n",
    "            \n",
    "        except Exception as e:\n",
    "            add_message(\"system\", f\"Error analyzing patient: {str(e)}\")\n",
    "            status_indicator.value = '<span style=\"color:red; font-weight:bold;\"></span> Error occurred'\n",
    "    \n",
    "    # Event handlers\n",
    "    def on_send_button_click(b):\n",
    "        query = chat_input.value\n",
    "        if query.strip():\n",
    "            add_message(\"user\", query)\n",
    "            chat_input.value = \"\"  # Clear input\n",
    "            process_query(query)\n",
    "    \n",
    "    def on_use_preset_click(b):\n",
    "        query = preset_queries.value\n",
    "        if query.strip():\n",
    "            add_message(\"user\", query)\n",
    "            process_query(query)\n",
    "    \n",
    "    def on_search_button_click(b):\n",
    "        patient_id = patient_search.value\n",
    "        if patient_id.strip():\n",
    "            add_message(\"user\", f\"Analyze patient with ID: {patient_id}\")\n",
    "            analyze_patient(patient_id)\n",
    "            patient_search.value = \"\"  # Clear input\n",
    "    \n",
    "    def on_refresh_button_click(b):\n",
    "        status_indicator.value = '<span style=\"color:orange; font-weight:bold;\"></span> Refreshing data...'\n",
    "        # Simulate refresh\n",
    "        import time\n",
    "        time.sleep(1)\n",
    "        status_indicator.value = '<span style=\"color:green; font-weight:bold;\"></span> Data refreshed'\n",
    "        add_message(\"system\", \"Medical database refreshed\")\n",
    "    \n",
    "    # Connect event handlers\n",
    "    send_button.on_click(on_send_button_click)\n",
    "    use_preset_button.on_click(on_use_preset_click)\n",
    "    search_button.on_click(on_search_button_click)\n",
    "    refresh_button.on_click(on_refresh_button_click)\n",
    "    \n",
    "    # Enter key should trigger send\n",
    "    def on_enter(widget):\n",
    "        query = chat_input.value\n",
    "        if query.strip():\n",
    "            add_message(\"user\", query)\n",
    "            chat_input.value = \"\"  # Clear input\n",
    "            process_query(query)\n",
    "    \n",
    "    chat_input.on_submit(on_enter)\n",
    "    \n",
    "    # Add welcome message\n",
    "    add_message(\"system\", \"Welcome to MedGraph Explorer! Ask me anything about medical data or analyze specific patients.\")\n",
    "    \n",
    "    return dashboard\n",
    "\n",
    "# Function to run the UI\n",
    "def launch_medgraph_explorer():\n",
    "    dashboard = create_improved_dashboard()\n",
    "    display(dashboard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T23:27:22.342949Z",
     "iopub.status.busy": "2025-03-09T23:27:22.342658Z",
     "iopub.status.idle": "2025-03-09T23:27:22.390721Z",
     "shell.execute_reply": "2025-03-09T23:27:22.389723Z",
     "shell.execute_reply.started": "2025-03-09T23:27:22.342926Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-45498ef97360>:280: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  chat_input.on_submit(on_enter)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9e3e1cfdd442658a6d6b7e907fb2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='\\n    <div style=\"background-color: #1A5276; color: white; padding: 20px; border-ra"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "launch_medgraph_explorer()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
